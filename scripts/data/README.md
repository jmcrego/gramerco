# Data preparation

Here are the indications to prepare the data from the *data.noise* file generated by the noiser scripts.

### Generate the triplets (*clean data*, *noisy data*, *TAGs*)

In order to generate the triplets, one must execute the script [generate_dataset.py](./generate_dataset.py):
```
python generate_dataset.py /path/to/data.noise -to /path/to/other/data
```

The result is the create is the creation of 3 text files:
- */path/to/other/data.fr*: clean sentences
- */path/to/other/data.noise.fr*: noisy sentences
- */path/to/other/data.tag.fr*: tags to apply to the noisy sequence to retrieve the clean one

They both contain the same number of lines so that they are correctly aligned.


### Tokenize/encode and binarize the data

```
python preprocess_dataset.py /path/to/other/data  -to /path/to/folder/bin
```

*data.fr* and *data.noise.fr* are tokenized with the *FlaubertTokenizer*, then binarized. A corresponding binary file is formated in such a way that:
* The 50 first bytes correspond to an utf-8 string of metadata of format *\<vector shape tuple>@\<vector dtype>*. Example: *(2, 820, 134)@int64*.
* The rest of the bytes are associated to the two vectors of values and masks.
* The *GramercoDataset* class in [data.py](./data.py) is able to deduce the data tensors given the bin data path.

*data.tag.fr* is encoded in a way so that every tag is associated to an integer. The given file is binarized as well.
